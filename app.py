import os
import time
import streamlit as st

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_core.messages import HumanMessage, AIMessage

# Streamlit í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ìƒë‹¨ì— ìœ„ì¹˜)
st.set_page_config(page_title="ë©”ëª¨ë¦¬ë„¤ë¹„ ğŸ’¬ğŸ“š", page_icon="ğŸ§­", layout="centered")

# OpenAI API í‚¤ ì„¤ì •
openai_api_key = os.getenv("MY_OPENAI_API_KEY")
if openai_api_key is None:
    st.error("ì˜¤ë¥˜: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'MY_OPENAI_API_KEY' í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()
os.environ["OPENAI_API_KEY"] = openai_api_key

# ìºì‹œ ë¦¬ì†ŒìŠ¤ë¡œ PDF ë¡œë“œ ë° ë¶„í• 
@st.cache_resource
def load_and_split_pdf(file_path):
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()

# ìºì‹œ ë¦¬ì†ŒìŠ¤ë¡œ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    split_docs = text_splitter.split_documents(_docs)
    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)
    vectorstore = FAISS.from_documents(split_docs, embeddings)
    vectorstore.save_local("faiss_index")
    return vectorstore

# ìºì‹œ ë¦¬ì†ŒìŠ¤ë¡œ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ë˜ëŠ” ìƒì„±
@st.cache_resource
def get_vectorstore(_docs):
    faiss_index_path = "faiss_index"
    if os.path.exists(os.path.join(faiss_index_path, "index.faiss")) and \
       os.path.exists(os.path.join(faiss_index_path, "index.pkl")):
        embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)
        return FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)
    else:
        return create_vector_store(_docs)

# ìºì‹œ ë¦¬ì†ŒìŠ¤ë¡œ LangChain ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
@st.cache_resource
def initialize_components(selected_model):
    data_dir = "./data"
    all_pages = []

    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
        st.info(f"'{data_dir}' ë””ë ‰í† ë¦¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ì´ ì•ˆì— ë„£ì–´ì£¼ì„¸ìš”.")
        return None 

    pdf_found = False
    for filename in os.listdir(data_dir):
        if filename.lower().endswith(".pdf"):
            file_path = os.path.join(data_dir, filename)
            try:
                pages = load_and_split_pdf(file_path)
                all_pages.extend(pages)
                pdf_found = True
            except Exception as e:
                st.warning(f"âš ï¸ {filename} ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")

    if not pdf_found:
        st.error("âŒ data í´ë”ì— ìœ íš¨í•œ PDF ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    vectorstore = get_vectorstore(all_pages)
    retriever = vectorstore.as_retriever()

    contextualize_q_system_prompt = """ì£¼ì–´ì§„ ì±„íŒ… íˆìŠ¤í† ë¦¬ì™€ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ,
        ì±„íŒ… íˆìŠ¤í† ë¦¬ ì—†ì´ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”.
        ì§ˆë¬¸ì— ë‹µí•˜ì§€ ë§ê³ , í•„ìš”ì‹œ ì¬êµ¬ì„±í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”."""
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    qa_system_prompt = """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

    ë‹¹ì‹ ì˜ ì£¼ìš” ì„ë¬´ëŠ” ì¹˜ë§¤ë¥¼ ì¤€ë¹„í•˜ëŠ” ë…¸ì¸ì„ ìœ„í•œ êµ­ê°€ ì§€ì› ì œë„, ë³µì§€ í˜œíƒ, ì˜ë£Œ ë° ëŒë´„ ì •ë³´ ë“± ì¹˜ë§¤ ì „ë°˜ì— ëŒ€í•œ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìœ ìµí•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 
    ë‹µë³€ì€ ê°„ê²°í•˜ê³  **ìµœëŒ€í•œ 8ì¤„ ì´ë‚´ë¡œ ì„¤ëª…**í•´ì£¼ê³ , ì–´ë¦°ì´ë„ ì´í•´í•  ìˆ˜ ìˆì„ ì •ë„ë¡œ ì‰¬ìš´ ì–¸ì–´ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

    --- 
    **ë‹µë³€ ê¸°ì¤€ ë° ê·œì¹™** 1. ì•„ë˜ì— ì œê³µëœ ë¬¸ì„œ(context)ê°€ ì¡´ì¬í•  ê²½ìš°
        - ë°˜ë“œì‹œ **context ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ** ë‹µë³€í•˜ì„¸ìš”. 
        - ì¼ë°˜ì ì¸ ë°°ê²½ì§€ì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
        - ì¶œì²˜ë‚˜ ìª½ìˆ˜ëŠ” **í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”.**
        - ì—°ë½ì²˜ë¥¼ ë¬¼ì–´ë³´ëŠ”ê²½ìš° ì—°ë½ì²˜ë¥¼ ì•ˆë‚´í•´ì£¼ì„¸ìš”. 
        

    2. ì•„ë˜ contextê°€ ë¹„ì–´ ìˆì„ ê²½ìš°
        - GPT ëª¨ë¸ì´ ì•Œê³  ìˆëŠ” ì¼ë°˜ì ì¸ ì§€ì‹ë§Œì„ ì‚¬ìš©í•´ ë‹µë³€í•˜ì„¸ìš”. 
        - ì´ ê²½ìš°, ë°˜ë“œì‹œ ë‹¤ìŒ ë¬¸ì¥ì„ **ë‹µë³€ì˜ ì²«ë¨¸ë¦¬ì— ì¤„ë°”ê¿ˆ 2ë²ˆ í›„ ì¶œë ¥**í•˜ì„¸ìš” : ì´ ë‹µë³€ì€ ì œê°€ ê°€ì§„ ì¼ë°˜ì ì¸ ì •ë³´ë¡œ ì•Œë ¤ ë“œë¦¬ëŠ” ê±°ì˜ˆìš”.

    3. ë¬¸ì„œì—ì„œ ì œê³µë˜ì§€ ì•ŠëŠ” ì •ë³´ì˜ ê²½ìš°
        - **ë‹µë³€ì˜ ì²«ë¨¸ë¦¬ì— ì¤„ë°”ê¿ˆ 2ë²ˆ í›„ ì¶œë ¥**í•˜ì„¸ìš” : ì´ ë‹µë³€ì€ ì œê°€ ê°€ì§„ ì¼ë°˜ì ì¸ ì •ë³´ë¡œ ì•Œë ¤ ë“œë¦¬ëŠ” ê±°ì˜ˆìš”.

          --- 
          ì°¸ê³  ë¬¸ì„œ (context): 
          {context}

      """
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    llm = ChatOpenAI(model=selected_model)
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    Youtube_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, Youtube_chain)

    return rag_chain

# Streamlit UI íƒ€ì´í‹€ ë° CSS
st.markdown("""
<div class="title-section">
    <h1>ğŸ§­ ë©”ëª¨ë¦¬ë„¤ë¹„</h1>
    <p>ì–´ë¥´ì‹ ì„ ìœ„í•œ ì¹˜ë§¤ ê´€ë ¨ ì •ë³´ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.<br>
    ê¶ê¸ˆí•œ ë‚´ìš©ì„ í¸í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!</p>
</div>
""", unsafe_allow_html=True)

st.markdown("""
<style>
/* ... (ê¸°ì¡´ CSS ìŠ¤íƒ€ì¼ ìœ ì§€) ... */
.stTextInput > div > input {
    font-size: 24px !important;
}
.title-section h1 {
    color: #000000 !important;
}
.title-section p {
    color: #555555 !important;
}
.title-section {
    text-align: center;
    background-color: #f8f9f9;
    padding: 1.2rem;
    border-radius: 12px;
    margin-bottom: 1.2rem;
    border: 1px solid #ddd;
}
div[data-testid="stChatMessage"][data-variant="assistant"] {
    font-size: 24px !important;
    line-height: 1.8 !important;
}
div[data-testid="stChatMessage"][data-variant="assistant"] p {
    font-size: 24px !important;
    line-height: 1.8 !important;
}
div[data-testid="stChatMessage"][data-variant="user"] {
    font-size: 24px !important;
    line-height: 1.6 !important;
}
div[data-testid="stChatMessage"][data-variant="user"] p {
    font-size: 24px !important;
    line-height: 1.6 !important;
}
.reference-docs-content,
.reference-docs-content p,
.reference-docs-content span,
.reference-docs-content li {
    font-size: 18px !important;
    line-height: 1.5 !important;
}
div[data-testid="stChatMessage"][data-variant="assistant"]:first-of-type {
    padding: 1.2rem 1.5rem !important;
    min-height: 96px !important;
}
[data-testid="stChatInput"] > div:first-child {
    min-height: 64px !important;
    display: flex;
    align-items: center;
    padding: 0 1rem !important;
}
[data-testid="stChatInput"] textarea {
    font-size: 20px !important;
    line-height: 1.6 !important;
    padding: 0.6rem 0.5rem !important;
}
[data-testid="stChatInput"] textarea::placeholder {
    font-size: 20px !important;
    opacity: 0.7;
}
</style>
""", unsafe_allow_html=True)


selected_model = "gpt-4o-mini"
rag_chain = initialize_components(selected_model)

# StreamlitChatMessageHistoryë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥í•˜ê³  ê´€ë¦¬
# ì´ ê°ì²´ê°€ st.session_state["chat_messages"]ì™€ ì—°ê²°ë©ë‹ˆë‹¤.
if "chat_history_obj" not in st.session_state:
    st.session_state.chat_history_obj = StreamlitChatMessageHistory(key="chat_messages")

# ì´ˆê¸° í™˜ì˜ ë©”ì‹œì§€ (StreamlitChatMessageHistoryì— ì—†ìœ¼ë©´ ì¶”ê°€)
if not st.session_state.chat_history_obj.messages:
    st.session_state.chat_history_obj.add_ai_message("ì¹˜ë§¤ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ§ âœ¨")

#    `st.chat_message`ëŠ” ë©”ì‹œì§€ë¥¼ í‘œì‹œí•˜ëŠ” ì—­í• ë§Œ í•˜ë„ë¡ í•©ë‹ˆë‹¤.
for msg in st.session_state.chat_history_obj.messages:
    if msg.type == "human":
        with st.chat_message("human"):
            st.markdown(f"<span style='font-size:24px; color:#007BFF;'>{msg.content}</span>", unsafe_allow_html=True)
    elif msg.type == "ai":
        if msg.content != "": # ë‚´ìš©ì´ ìˆëŠ” AI ë©”ì‹œì§€ë§Œ ê·¸ë¦½ë‹ˆë‹¤.
            with st.chat_message("ai"):
                st.markdown(f"<span style='font-size:24px;'>{msg.content}</span>", unsafe_allow_html=True)


# rag_chainì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆì„ ë•Œë§Œ conversational_rag_chainì„ ìƒì„±
if rag_chain:
    conversational_rag_chain = RunnableWithMessageHistory(
        rag_chain,
        lambda session_id: st.session_state.chat_history_obj, # ì„¸ì…˜ì— ì €ì¥ëœ StreamlitChatMessageHistory ê°ì²´ ì‚¬ìš©
        input_messages_key="input",
        history_messages_key="history",
        output_messages_key="answer",
    )
else:
    st.info("PDF ë¬¸ì„œê°€ ì—†ê±°ë‚˜ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ ì±—ë´‡ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'data' í´ë”ì— PDF íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
    conversational_rag_chain = None


# ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ëŠ” ë¶€ë¶„
if prompt_message := st.chat_input("ì¹˜ë§¤ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì—¬ê¸°ì— ì…ë ¥í•´ ì£¼ì„¸ìš”."):
    if conversational_rag_chain:
        # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ chat_history_objì— ì¶”ê°€
        st.session_state.chat_history_obj.add_user_message(prompt_message)
        
        with st.chat_message("ai"):
            message_placeholder = st.empty() # ì´ placeholderì— ì‘ë‹µì„ ì ì§„ì ìœ¼ë¡œ í‘œì‹œ

            full_response = ""
            with st.spinner("ìƒê° ì¤‘ì…ë‹ˆë‹¤... ğŸ§"):
                config = {"configurable": {"session_id": "any"}}
                
                response = conversational_rag_chain.invoke(
                    {"input": prompt_message}, 
                    config
                )
                answer = response['answer']

                # íƒ€ì´í•‘ íš¨ê³¼
                for chunk in answer.split(" "):
                    full_response += chunk + " "
                    message_placeholder.markdown(f"<span style='font-size:24px;'>{full_response}</span>", unsafe_allow_html=True)
                    time.sleep(0.05)

# chat_history ëŒ€ì‹  ì¼ë°˜ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ˆê¸° í™˜ì˜ ë©”ì‹œì§€ (ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ ì¶”ê°€)
if not st.session_state.messages:
    st.session_state.messages.append(AIMessage(content="ì¹˜ë§¤ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ§ âœ¨"))

# ì´ì „ ëŒ€í™” ë©”ì‹œì§€ ì¶œë ¥ (st.session_state.messagesì—ì„œ ì½ì–´ì„œ í‘œì‹œ)
for msg in st.session_state.messages:
    if msg.type == "human":
        with st.chat_message("human"):
            st.markdown(f"<span style='font-size:24px; color:#007BFF;'>{msg.content}</span>", unsafe_allow_html=True)
    else: # msg.type == "ai"
        # AI ì‘ë‹µì´ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í‘œì‹œ (íƒ€ì´í•‘ íš¨ê³¼ ì¤‘ ë¹ˆ ë²„ë¸” ì¤‘ë³µ ë°©ì§€)
        if msg.content != "":
            with st.chat_message("ai"):
                st.markdown(f"<span style='font-size:24px;'>{msg.content}</span>", unsafe_allow_html=True)

# rag_chainì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆì„ ë•Œë§Œ conversational_rag_chainì„ ìƒì„±
if rag_chain:
    # StreamlitChatMessageHistory ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì„¸ì…˜ ìƒíƒœì— ìœ ì§€
    if "chat_history_obj" not in st.session_state:
        st.session_state.chat_history_obj = StreamlitChatMessageHistory(key="chat_messages")

    conversational_rag_chain = RunnableWithMessageHistory(
        rag_chain,
        lambda session_id: st.session_state.chat_history_obj, # ì„¸ì…˜ì— ì €ì¥ëœ ê°ì²´ ì‚¬ìš©
        input_messages_key="input",
        history_messages_key="history",
        output_messages_key="answer",
    )
else:
    st.info("PDF ë¬¸ì„œê°€ ì—†ê±°ë‚˜ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ ì±—ë´‡ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'data' í´ë”ì— PDF íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
    conversational_rag_chain = None

if not st.session_state.chat_history_obj.messages:
    st.session_state.chat_history_obj.add_ai_message("ì¹˜ë§¤ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ§ âœ¨")

for msg in st.session_state.chat_history_obj.messages:
  
    if msg.type == "human":
        with st.chat_message("human"):
            st.markdown(f"<span style='font-size:24px; color:#007BFF;'>{msg.content}</span>", unsafe_allow_html=True)
    elif msg.type == "ai" and msg.content != "": # ë‚´ìš©ì´ ìˆëŠ” AI ë©”ì‹œì§€ë§Œ ê·¸ë¦½ë‹ˆë‹¤.
        with st.chat_message("ai"):
            st.markdown(f"<span style='font-size:24px;'>{msg.content}</span>", unsafe_allow_html=True)


# ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ëŠ” ë¶€ë¶„
if prompt_message := st.chat_input("ì¹˜ë§¤ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì—¬ê¸°ì— ì…ë ¥í•´ ì£¼ì„¸ìš”."):
    if conversational_rag_chain:
        # 1. ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ chat_history_objì— ì¶”ê°€
        st.session_state.chat_history_obj.add_user_message(prompt_message)
        
        st.session_state.chat_history_obj.add_ai_message("") 
        
        # 3. ì•±ì„ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ì‚¬ìš©ì ë©”ì‹œì§€(ìƒˆë¡œ ì¶”ê°€ëœ ê²ƒ)ê°€ í‘œì‹œë˜ë„ë¡ í•©ë‹ˆë‹¤.
        #    ì´ë•Œ ë¹ˆ AI ë²„ë¸”ì€ ìœ„ ë Œë”ë§ ë£¨í”„ì—ì„œ ê±´ë„ˆë›°ì–´ì ¸ì„œ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.
        st.rerun() 

if st.session_state.chat_history_obj.messages and \
   st.session_state.chat_history_obj.messages[-1].type == "ai" and \
   st.session_state.chat_history_obj.messages[-1].content == "":
    
    with st.chat_message("ai"):
        message_placeholder = st.empty() 
        full_response = ""

        with st.spinner("ìƒê° ì¤‘ì…ë‹ˆë‹¤... ğŸ§"):
            config = {"configurable": {"session_id": "any"}}
            
            # conversational_rag_chain.invoke í˜¸ì¶œ ì‹œ, LangChainì´ ë‚´ë¶€ì ìœ¼ë¡œ
            # st.session_state.chat_history_objë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
            response = conversational_rag_chain.invoke(
                {"input": st.session_state.chat_history_obj.messages[-2].content}, # ê°€ì¥ ìµœê·¼ ì‚¬ìš©ì ë©”ì‹œì§€
                config
            )
            answer = response['answer']

            # íƒ€ì´í•‘ íš¨ê³¼
            for chunk in answer.split(" "):
                full_response += chunk + " "
                message_placeholder.markdown(f"<span style='font-size:24px;'>{full_response}</span>", unsafe_allow_html=True)
                time.sleep(0.05)
                
            st.session_state.chat_history_obj.messages[-1].content = answer

        # ì°¸ê³  ë¬¸ì„œ ìœ ì‚¬ë„ í•„í„°ë§ ë° ì¶œë ¥
        embeddings_for_score = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)
        vectorstore_for_score = get_vectorstore([])
        
        # ê°€ì¥ ìµœê·¼ ì‚¬ìš©ì ë©”ì‹œì§€ë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰
        scored_docs = vectorstore_for_score.similarity_search_with_score(st.session_state.chat_history_obj.messages[-2].content, k=3)

        filtered_docs = []
        for doc, score in scored_docs:
            sim_score = 1 - score / 2 # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì ìˆ˜ ë³€í™˜
            if sim_score >= 0.4:
                filtered_docs.append(doc)

        if filtered_docs:
            with st.expander("ğŸ” ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                st.markdown("<div class='reference-docs-content'>", unsafe_allow_html=True)
                for i, doc in enumerate(filtered_docs):
                    source = os.path.basename(doc.metadata.get("source", ""))
                    page = doc.metadata.get("page", None)
                    st.markdown(f"**ë¬¸ì„œ {i+1}:**")
                    if source and page is not None:
                        st.markdown(f"- ğŸ“„ {source} - {page + 1}ìª½")
                    else:
                        st.markdown("- â” ì¶œì²˜ ì—†ìŒ")
                    st.write(doc.page_content)
                    st.markdown("---")
                st.markdown("</div>", unsafe_allow_html=True)
