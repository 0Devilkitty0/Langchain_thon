import os
import time
import streamlit as st

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_core.messages import HumanMessage, AIMessage 

# Streamlit í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ìƒë‹¨ì— ìœ„ì¹˜)
st.set_page_config(page_title="ë©”ëª¨ë¦¬ë„¤ë¹„ ğŸ’¬ğŸ“š", page_icon="ğŸ§­", layout="centered")

# OpenAI API í‚¤ ì„¤ì •
# í™˜ê²½ ë³€ìˆ˜ MY_OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥ í›„ ì•± ì¤‘ë‹¨
openai_api_key = os.getenv("MY_OPENAI_API_KEY")
if openai_api_key is None:
    st.error("ì˜¤ë¥˜: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'MY_OPENAI_API_KEY' í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop() # API í‚¤ê°€ ì—†ìœ¼ë©´ ì•± ì‹¤í–‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
os.environ["OPENAI_API_KEY"] = openai_api_key

# @st.cache_resource: í•œ ë²ˆ ì‹¤í–‰ëœ ê²°ê³¼ë¥¼ ìºì‹±í•˜ì—¬ ì¬ì‹¤í–‰ ì‹œ ì‹œê°„ì„ ì ˆì•½
@st.cache_resource
def load_and_split_pdf(file_path):
    """PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  í˜ì´ì§€ë³„ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()

@st.cache_resource
def create_vector_store(_docs):
    """ë¬¸ì„œ ì²­í¬ë¥¼ FAISS ë²¡í„°ìŠ¤í† ì–´ì— ì„ë² ë”©í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."""
    # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •: ì²­í¬ í¬ê¸° 500, ì¤‘ë³µ 100
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    split_docs = text_splitter.split_documents(_docs)

    # OpenAI ì„ë² ë”© ëª¨ë¸ ì„¤ì •: text-embedding-3-small, ì°¨ì› 1536 ëª…ì‹œ (FAISS ì°¨ì› ë¶ˆì¼ì¹˜ ë°©ì§€)
    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)
    # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    vectorstore = FAISS.from_documents(split_docs, embeddings)

    # ë¡œì»¬ì— FAISS ì¸ë±ìŠ¤ ì €ì¥
    vectorstore.save_local("faiss_index")
    return vectorstore

@st.cache_resource
def get_vectorstore(_docs):
    """ë¡œì»¬ì— ì €ì¥ëœ FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•˜ê±°ë‚˜, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
    faiss_index_path = "faiss_index"
    # ì¸ë±ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ë¡œë“œ
    if os.path.exists(os.path.join(faiss_index_path, "index.faiss")) and \
       os.path.exists(os.path.join(faiss_index_path, "index.pkl")):
        embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)
        return FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)
    # ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    else:
        return create_vector_store(_docs)


@st.cache_resource
def initialize_components(selected_model):
    """
    data í´ë”ì˜ ëª¨ë“  PDFë¥¼ ë¡œë“œí•˜ì—¬ ë²¡í„° DBë¥¼ ë§Œë“¤ê³ ,
    ê²€ìƒ‰ ë° ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ ì „ì²´ LangChain ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
    """
    data_dir = "./data"
    all_pages = []

    # 'data' ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±í•˜ê³  ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
        st.info(f"'{data_dir}' ë””ë ‰í† ë¦¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ì´ ì•ˆì— ë„£ì–´ì£¼ì„¸ìš”.")
        return None # PDF íŒŒì¼ì´ ì—†ìœ¼ë¯€ë¡œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘ë‹¨

    pdf_found = False
    # data ë””ë ‰í† ë¦¬ ë‚´ì˜ ëª¨ë“  PDF íŒŒì¼ì„ ë¡œë“œ
    for filename in os.listdir(data_dir):
        if filename.lower().endswith(".pdf"):
            file_path = os.path.join(data_dir, filename)
            try:
                pages = load_and_split_pdf(file_path)
                all_pages.extend(pages)
                pdf_found = True
            except Exception as e:
                st.warning(f"âš ï¸ {filename} ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")

    # ìœ íš¨í•œ PDF ë¬¸ì„œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
    if not pdf_found:
        st.error(f"âŒ '{data_dir}' í´ë”ì— ìœ íš¨í•œ PDF ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° retriever ì¶”ì¶œ
    vectorstore = get_vectorstore(all_pages)
    retriever = vectorstore.as_retriever()

    # íˆìŠ¤í† ë¦¬ ê¸°ë°˜ retrieverë¥¼ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜
    contextualize_q_system_prompt = """ì£¼ì–´ì§„ ì±„íŒ… íˆìŠ¤í† ë¦¬ì™€ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ,
        ì±„íŒ… íˆìŠ¤í† ë¦¬ ì—†ì´ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”.
        ì§ˆë¬¸ì— ë‹µí•˜ì§€ ë§ê³ , í•„ìš”ì‹œ ì¬êµ¬ì„±í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”."""

    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"), # ì±„íŒ… íˆìŠ¤í† ë¦¬ í”Œë ˆì´ìŠ¤í™€ë”
            ("human", "{input}"), # ì‚¬ìš©ì ì…ë ¥ í”Œë ˆì´ìŠ¤í™€ë”
        ]
    )

    # QA ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜ (ë‹µë³€ ê·œì¹™ í¬í•¨)
    qa_system_prompt = """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

    ë‹¹ì‹ ì˜ ì£¼ìš” ì„ë¬´ëŠ” ì¹˜ë§¤ë¥¼ ì¤€ë¹„í•˜ëŠ” ë…¸ì¸ì„ ìœ„í•œ êµ­ê°€ ì§€ì› ì œë„, ë³µì§€ í˜œíƒ, ì˜ë£Œ ë° ëŒë´„ ì •ë³´ ë“± ì¹˜ë§¤ ì „ë°˜ì— ëŒ€í•œ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìœ ìµí•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 
    ë‹µë³€ì€ ê°„ê²°í•˜ê³  **ìµœëŒ€í•œ 8ì¤„ ì´ë‚´ë¡œ ì„¤ëª…**í•´ì£¼ê³ , ì–´ë¦°ì´ë„ ì´í•´í•  ìˆ˜ ìˆì„ ì •ë„ë¡œ ì‰¬ìš´ ì–¸ì–´ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

    --- 
    **ë‹µë³€ ê¸°ì¤€ ë° ê·œì¹™** 1. ì•„ë˜ì— ì œê³µëœ ë¬¸ì„œ(context)ê°€ ì¡´ì¬í•  ê²½ìš°
        - ë°˜ë“œì‹œ **context ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ** ë‹µë³€í•˜ì„¸ìš”. 
        - ì¼ë°˜ì ì¸ ë°°ê²½ì§€ì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
        - ì¶œì²˜ë‚˜ ìª½ìˆ˜ëŠ” **í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”.**
        - ì—°ë½ì²˜ë¥¼ ë¬¼ì–´ë³´ëŠ”ê²½ìš° ì—°ë½ì²˜ë¥¼ ì•ˆë‚´í•´ì£¼ì„¸ìš”. 
        

    2. ì•„ë˜ contextê°€ ë¹„ì–´ ìˆì„ ê²½ìš°
        - GPT ëª¨ë¸ì´ ì•Œê³  ìˆëŠ” ì¼ë°˜ì ì¸ ì§€ì‹ë§Œì„ ì‚¬ìš©í•´ ë‹µë³€í•˜ì„¸ìš”. 
        - ì´ ê²½ìš°, ë°˜ë“œì‹œ ë‹¤ìŒ ë¬¸ì¥ì„ **ë‹µë³€ì˜ ì²«ë¨¸ë¦¬ì— ì¤„ë°”ê¿ˆ 2ë²ˆ í›„ ì¶œë ¥**í•˜ì„¸ìš” : ì´ ë‹µë³€ì€ ì œê°€ ê°€ì§„ ì¼ë°˜ì ì¸ ì •ë³´ë¡œ ì•Œë ¤ ë“œë¦¬ëŠ” ê±°ì˜ˆìš”.

    3. ë¬¸ì„œì—ì„œ ì œê³µë˜ì§€ ì•ŠëŠ” ì •ë³´ì˜ ê²½ìš° (ì´ ê·œì¹™ì€ 2ë²ˆê³¼ ìœ ì‚¬í•˜ê²Œ ì‘ë™)
        - **ë‹µë³€ì˜ ì²«ë¨¸ë¦¬ì— ì¤„ë°”ê¿ˆ 2ë²ˆ í›„ ì¶œë ¥**í•˜ì„¸ìš” : ì´ ë‹µë³€ì€ ì œê°€ ê°€ì§„ ì¼ë°˜ì ì¸ ì •ë³´ë¡œ ì•Œë ¤ ë“œë¦¬ëŠ” ê±°ì˜ˆìš”.

          --- 
          ì°¸ê³  ë¬¸ì„œ (context): 
          {context}

      """

    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"), # ì±„íŒ… íˆìŠ¤í† ë¦¬ í”Œë ˆì´ìŠ¤í™€ë”
            ("human", "{input}"), # ì‚¬ìš©ì ì…ë ¥ í”Œë ˆì´ìŠ¤í™€ë”
        ]
    )

    # LLM, íˆìŠ¤í† ë¦¬ ì¸ì‹ ë¦¬íŠ¸ë¦¬ë²„, QA ì²´ì¸ êµ¬ì„±
    llm = ChatOpenAI(model=selected_model)
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    # ìµœì¢… RAG ì²´ì¸ ë°˜í™˜
    return rag_chain


# Streamlit UI ì‹œì‘
st.markdown("""
<div class="title-section">
    <h1>ğŸ§­ ë©”ëª¨ë¦¬ë„¤ë¹„</h1>
    <p>ì–´ë¥´ì‹ ì„ ìœ„í•œ ì¹˜ë§¤ ê´€ë ¨ ì •ë³´ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.<br>
    ê¶ê¸ˆí•œ ë‚´ìš©ì„ í¸í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!</p>
</div>
""", unsafe_allow_html=True)

# CSS ìŠ¤íƒ€ì¼ ì •ì˜
st.markdown("""
<style>
/* ì‚¬ìš©ì ì…ë ¥ì°½ ê¸€ì í¬ê¸° */
.stTextInput > div > input {
    font-size: 24px !important;
}

/* íƒ€ì´í‹€ ì˜ì—­ ìŠ¤íƒ€ì¼ */
.title-section h1 {
    color: #000000 !important;
}
.title-section p {
    color: #555555 !important;
}
.title-section {
    text-align: center;
    background-color: #f8f9f9;
    padding: 1.2rem;
    border-radius: 12px;
    margin-bottom: 1.2rem;
    border: 1px solid #ddd;
}

/* AI ë©”ì‹œì§€ ë²„ë¸” ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸ì™€ <p> íƒœê·¸ í°íŠ¸ í¬ê¸° */
div[data-testid="stChatMessage"][data-variant="assistant"] {
    font-size: 24px !important; /* AI ë©”ì‹œì§€ ë²„ë¸” ì „ì²´ì˜ ê¸°ë³¸ í°íŠ¸ í¬ê¸° */
    line-height: 1.8 !important;
}
div[data-testid="stChatMessage"][data-variant="assistant"] p {
    font-size: 24px !important; /* AI ë©”ì‹œì§€ ë²„ë¸” ë‚´ë¶€ <p> íƒœê·¸ì˜ í°íŠ¸ í¬ê¸° */
    line-height: 1.8 !important;
}

/* ì‚¬ìš©ì ë©”ì‹œì§€ ë²„ë¸” ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸ì™€ <p> íƒœê·¸ í°íŠ¸ í¬ê¸° */
div[data-testid="stChatMessage"][data-variant="user"] {
    font-size: 24px !important; /* ì‚¬ìš©ì ë©”ì‹œì§€ ë²„ë¸” ì „ì²´ì˜ ê¸°ë³¸ í°íŠ¸ í¬ê¸° */
    line-height: 1.6 !important;
}
div[data-testid="stChatMessage"][data-variant="user"] p {
    font-size: 24px !important; /* ì‚¬ìš©ì ë©”ì‹œì§€ ë²„ë¸” ë‚´ë¶€ <p> íƒœê·¸ì˜ í°íŠ¸ í¬ê¸° */
    line-height: 1.6 !important;
}
            
/* ì°¸ê³  ë¬¸ì„œ í™•ì¸ í°íŠ¸ í¬ê¸° ì¡°ì ˆ */
.reference-docs-content,
.reference-docs-content p,
.reference-docs-content span,
.reference-docs-content li { /* ë¦¬ìŠ¤íŠ¸ í•­ëª©ë„ í¬í•¨ */
    font-size: 18px !important; /* ì°¸ê³  ë¬¸ì„œ í°íŠ¸ í¬ê¸° */
    line-height: 1.5 !important;
}

/* ì´ˆê¸° AI ë²„ë¸”(â€œì¹˜ë§¤ì— ëŒ€í•´ ë¬´ì—‡ì´ë“ â€¦â€) ì „ì²´ ë†’ì´Â·ì—¬ë°± í™•ëŒ€ */
div[data-testid="stChatMessage"][data-variant="assistant"]:first-of-type {
    padding: 1.2rem 1.5rem !important;  /* ìƒí•˜Â·ì¢Œìš° ì—¬ë°± â†‘ */
    min-height: 96px !important;        /* ì¹¸ ìì²´ ê¸°ë³¸ ë†’ì´ â†‘ */
}
            
/* ì…ë ¥ì°½(placeholder í¬í•¨) */
/* ChatInput ë°•ìŠ¤ ìì²´ ë†’ì´ ëŠ˜ë¦¬ê¸° */
[data-testid="stChatInput"] > div:first-child {
    min-height: 64px !important;    /* ì›í•˜ëŠ” ë†’ì´ (px) */
    display: flex;
    align-items: center;            /* ì„¸ë¡œ ê°€ìš´ë° ì •ë ¬ */
    padding: 0 1rem !important;
}
/* ì‹¤ì œ ì…ë ¥ textarea */
[data-testid="stChatInput"] textarea {
    font-size: 20px !important;     /* ì…ë ¥ ê¸€ì í¬ê¸° */
    line-height: 1.6 !important;
    padding: 0.6rem 0.5rem !important;
}
/* placeholder ê¸€ê¼´ë„ ë™ì¼í•˜ê²Œ */
[data-testid="stChatInput"] textarea::placeholder {
    font-size: 20px !important;
    opacity: 0.7;                   /* ì‚´ì§ ì˜…ê²Œ ë³´ì´ë„ë¡ */
}
</style>
""", unsafe_allow_html=True)


# ëª¨ë¸ì„ gpt-4o-minië¡œ ê³ ì • (selectbox ì œê±°)
selected_model = "gpt-4o-mini"
rag_chain = initialize_components(selected_model) # initialize_components í•¨ìˆ˜ì— ê³ ì •ëœ ëª¨ë¸ ì „ë‹¬

# StreamlitChatMessageHistory ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
chat_history = StreamlitChatMessageHistory(key="chat_messages")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ë¬¸ë§¥/ì±„íŒ… ë©”ì‹œì§€)
if "context" not in st.session_state:
    st.session_state["context"] = []

# ì´ˆê¸° í™˜ì˜ ë©”ì‹œì§€ (íˆìŠ¤í† ë¦¬ì— ì—†ìœ¼ë©´ ì¶”ê°€)
if not chat_history.messages:
    chat_history.add_ai_message("ì¹˜ë§¤ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸ§ âœ¨")

# ì´ì „ ëŒ€í™” ë©”ì‹œì§€ ì¶œë ¥ (íˆìŠ¤í† ë¦¬ì—ì„œë§Œ ì½ì–´ì„œ í‘œì‹œ)
# ì´ ë£¨í”„ëŠ” ì•±ì´ ìƒˆë¡œê³ ì¹¨ë  ë•Œë§ˆë‹¤ ëª¨ë“  ë©”ì‹œì§€ë¥¼ ë‹¤ì‹œ ê·¸ë¦½ë‹ˆë‹¤.
for msg in chat_history.messages:
    if msg.type == "human":
        with st.chat_message("human"):
            st.markdown(f"<span style='font-size:24px; color:#007BFF;'>{msg.content}</span>", unsafe_allow_html=True)
    else: # msg.type == "ai"
        with st.chat_message("ai"):
            # AI ë©”ì‹œì§€ëŠ” CSS ê·œì¹™ì— ë”°ë¼ í°íŠ¸ í¬ê¸° 24px ì ìš©
            st.markdown(f"<span style='font-size:24px;'>{msg.content}</span>", unsafe_allow_html=True)


# rag_chainì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆì„ ë•Œë§Œ conversational_rag_chainì„ ìƒì„±
if rag_chain:
    # RunnableWithMessageHistoryëŠ” ë‚´ë¶€ì ìœ¼ë¡œ chat_historyë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    # ì´ ê°ì²´ë¥¼ í†µí•´ LangChain ì²´ì¸ì— ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
    conversational_rag_chain = RunnableWithMessageHistory(
        rag_chain,
        lambda session_id: chat_history, # ì„¸ì…˜ IDì— ë”°ë¼ chat_history ê°ì²´ ë°˜í™˜
        input_messages_key="input",      # ì‚¬ìš©ì ì…ë ¥ì´ ì „ë‹¬ë  í‚¤
        history_messages_key="history",  # ì±„íŒ… íˆìŠ¤í† ë¦¬ê°€ ì „ë‹¬ë  í‚¤
        output_messages_key="answer",    # LLM ì‘ë‹µì´ ë°˜í™˜ë  í‚¤
    )
else:
    st.info("PDF ë¬¸ì„œê°€ ì—†ê±°ë‚˜ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ ì±—ë´‡ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'data' í´ë”ì— PDF íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
    conversational_rag_chain = None # ì±—ë´‡ ë¹„í™œì„±í™”


# ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ëŠ” ë¶€ë¶„
# st.chat_inputì€ ì‚¬ìš©ìê°€ ì…ë ¥ì„ ì œì¶œí•˜ë©´ Trueë¥¼ ë°˜í™˜í•˜ê³ , prompt_messageì— ì…ë ¥ê°’ì„ í• ë‹¹í•©ë‹ˆë‹¤.
if prompt_message := st.chat_input("ì¹˜ë§¤ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì—¬ê¸°ì— ì…ë ¥í•´ ì£¼ì„¸ìš”."):
    if conversational_rag_chain: # ì±—ë´‡ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ì§ˆë¬¸ ì²˜ë¦¬
        # 1. ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ chat_historyì— ì¶”ê°€
        #    ì´ ë©”ì‹œì§€ëŠ” ë‹¤ìŒ st.rerun() ì‹œì ì— ìœ„ for ë£¨í”„ì— ì˜í•´ í™”ë©´ì— ê·¸ë ¤ì§‘ë‹ˆë‹¤.
        chat_history.add_user_message(prompt_message)
        
        # 2. AI ì‘ë‹µì„ ìœ„í•œ ë¹ˆ í”Œë ˆì´ìŠ¤í™€ë” ë©”ì‹œì§€ë¥¼ chat_historyì— ì¶”ê°€
        #    ì´ ë©”ì‹œì§€ ì—­ì‹œ ë‹¤ìŒ st.rerun() ì‹œì ì— ë¹ˆ AI ë²„ë¸”ë¡œ í™”ë©´ì— ê·¸ë ¤ì§‘ë‹ˆë‹¤.
        chat_history.add_ai_message("") 
        
        # 3. ì•±ì„ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ì‚¬ìš©ì ë©”ì‹œì§€ì™€ ë¹ˆ AI ë²„ë¸”ì„ ì¦‰ì‹œ í‘œì‹œ
        #    ì´ê²ƒì´ ì—†ìœ¼ë©´ AI ì‘ë‹µì´ ë‚˜ì˜¬ ë•Œê¹Œì§€ ì‚¬ìš©ì ë©”ì‹œì§€ê°€ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.
        st.rerun() 

# --- ì´ ë¶€ë¶„ì€ `st.chat_input` ë¸”ë¡ ë°–ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ---
# Streamlitì€ ì‚¬ìš©ì ì…ë ¥ í›„ ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ë¯€ë¡œ,
# ì´ ì¡°ê±´ë¬¸ì€ ì•±ì´ ì¬ì‹¤í–‰ë  ë•Œë§ˆë‹¤ ê²€ì‚¬ë©ë‹ˆë‹¤.
# ì´ ë¡œì§ì€ ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ 'ë¹„ì–´ìˆëŠ” AI ë©”ì‹œì§€'ì¼ ë•Œë§Œ ì‹¤í–‰ë˜ì–´,
# í•´ë‹¹ ë©”ì‹œì§€ë¥¼ LLM ì‘ë‹µìœ¼ë¡œ ì±„ìš°ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
if chat_history.messages and \
   chat_history.messages[-1].type == "ai" and \
   chat_history.messages[-1].content == "":
    
    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆëŠ” AI ë©”ì‹œì§€ í”Œë ˆì´ìŠ¤í™€ë”ë¼ë©´
    # ì´ ë©”ì‹œì§€ì— ì‘ë‹µì„ ìƒì„±í•˜ê³  íƒ€ì´í•‘ íš¨ê³¼ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
    with st.chat_message("ai"):
        # `st.empty()`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ ë‚´ìš©ì„ ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•  í”Œë ˆì´ìŠ¤í™€ë” ìƒì„±
        message_placeholder = st.empty()
        full_response = ""

        with st.spinner("ìƒê° ì¤‘ì…ë‹ˆë‹¤... ğŸ§"):
            # LangChain ì²´ì¸ í˜¸ì¶œ
            # `conversational_rag_chain.invoke`ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ `chat_history`ë¥¼ ì°¸ì¡°í•˜ì—¬
            # ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ LLMì— ì „ë‹¬í•©ë‹ˆë‹¤.
            config = {"configurable": {"session_id": "any"}}
            response = conversational_rag_chain.invoke(
                {"input": chat_history.messages[-2].content}, # ê°€ì¥ ìµœê·¼ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
                config
            )
            answer = response['answer']

            # íƒ€ì´í•‘ íš¨ê³¼ êµ¬í˜„
            for chunk in answer.split(" "): # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ëŠì–´ì„œ íƒ€ì´í•‘ íš¨ê³¼
                full_response += chunk + " "
                # `message_placeholder`ë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ì ì§„ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ í‘œì‹œ
                message_placeholder.markdown(f"<span style='font-size:24px;'>{full_response}</span>", unsafe_allow_html=True)
                time.sleep(0.05) # íƒ€ì´í•‘ ì†ë„ ì¡°ì ˆ (0.01~0.05 ì •ë„ê°€ ì ë‹¹)

            # ìµœì¢… ë‹µë³€ì„ chat_historyì˜ ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ì— ì—…ë°ì´íŠ¸
            # ì´ë ‡ê²Œ í•˜ë©´ ë‹¤ìŒ ìƒˆë¡œê³ ì¹¨ ì‹œ ì´ ë©”ì‹œì§€ê°€ ì™„ì „í•œ í˜•íƒœë¡œ í‘œì‹œë©ë‹ˆë‹¤.
            chat_history.messages[-1].content = answer

        # ì°¸ê³  ë¬¸ì„œ ìœ ì‚¬ë„ í•„í„°ë§ ë° ì¶œë ¥ (ìœ ì‚¬ë„ 0.4 ì´ìƒë§Œ)
        # ì´ ë¶€ë¶„ì€ `rag_chain`ì˜ `response`ì— í¬í•¨ëœ `context`ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì…ë‹ˆë‹¤.
        # í˜„ì¬ ì½”ë“œëŠ” `get_vectorstore([])`ë¥¼ ë‹¤ì‹œ í˜¸ì¶œí•˜ì—¬ ë³„ë„ë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤.
        # ì´ëŠ” ì¤‘ë³µ ì‘ì—…ì´ë©°, `rag_chain`ì´ ì´ë¯¸ ê²€ìƒ‰í•œ ë¬¸ì„œë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
        # í•˜ì§€ë§Œ í˜„ì¬ ì½”ë“œì˜ ë¡œì§ì„ ìœ ì§€í•˜ë©° ìŠ¤íƒ€ì¼ë§Œ ì ìš©í•©ë‹ˆë‹¤.
        
        embeddings_for_score = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)
        vectorstore_for_score = get_vectorstore([]) # ìºì‹œëœ ë‹¨ì¼ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„
        
        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ì‹œ ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
        scored_docs = vectorstore_for_score.similarity_search_with_score(chat_history.messages[-2].content, k=3)

        filtered_docs = []
        for doc, score in scored_docs:
            sim_score = 1 - score / 2 # FAISS (cosine) ê¸°ì¤€ ë³€í™˜ (ë‚®ì€ ì ìˆ˜ê°€ ë†’ì€ ìœ ì‚¬ë„)
            if sim_score >= 0.4: # ìœ ì‚¬ë„ 0.4 ì´ìƒë§Œ í•„í„°ë§ (ë†’ì€ ìœ ì‚¬ë„)
                filtered_docs.append(doc)

        # ë²„íŠ¼(Expander) ëˆŒë €ì„ ë•Œë§Œ í‘œì‹œ
        if filtered_docs:
            with st.expander("ğŸ” ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                # ì°¸ê³  ë¬¸ì„œ ë‚´ìš©ì„ ê°ì‹¸ëŠ” div ì¶”ê°€ (CSS ì ìš©ì„ ìœ„í•¨)
                st.markdown("<div class='reference-docs-content'>", unsafe_allow_html=True)
                for i, doc in enumerate(filtered_docs):
                    source = os.path.basename(doc.metadata.get("source", ""))
                    page = doc.metadata.get("page", None)
                    st.markdown(f"**ë¬¸ì„œ {i+1}:**") # ë¬¸ì„œ ë²ˆí˜¸ í‘œì‹œ
                    if source and page is not None:
                        st.markdown(f"- ğŸ“„ {source} - {page + 1}ìª½")
                    else:
                        st.markdown("- â” ì¶œì²˜ ì—†ìŒ")
                    st.write(doc.page_content) # ì°¸ê³  ë¬¸ì„œ ë‚´ìš© ì¶œë ¥
                    st.markdown("---")
                st.markdown("</div>", unsafe_allow_html=True) 
